{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision \n",
    "from torchvision.transforms import v2 \n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELUActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GELUActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self , config):\n",
    "        super().__init__()\n",
    "        self.img_size = config.image_size \n",
    "        self.patch_size = config.patch_size\n",
    "        self.in_channels = config.in_channels\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_patches = (self.img_size // self.patch_size) * (self.img_size // self.patch_size)\n",
    "        self.project = nn.Conv2d(self.in_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.project(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "    \n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config.hidden_size))\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size = x.shape[0]\n",
    "        clas_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((clas_tokens, x), dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size , attention_head_size, dropout , bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Attention scores = softmax (Q * K. T/ sqrt (head_size) )*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        self.qkv_bias = config.qkv_bias\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config.attention_probs_dropout_prob,\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        self.dense = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        \n",
    "    def forward(self, x , output_attentions=False):\n",
    "        \n",
    "        attention_outputs = []\n",
    "        for attention_head in self.heads:\n",
    "            attention_output = attention_head(x)\n",
    "            attention_outputs.append(attention_output)\n",
    "            \n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        attention_output = self.dense(attention_output)\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        \n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.intermediate_size)\n",
    "        self.fc2 = nn.Linear(self.intermediate_size, self.hidden_size)\n",
    "        self.activation = GELUActivation()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x, output_attentions=False):\n",
    "        attention_output, attention_probs = self.attention(self.layernorm1(x), output_attentions)\n",
    "        x = x + attention_output\n",
    "        mlp_output = self.mlp(self.layernorm2(x))\n",
    "        x = x + mlp_output\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "        \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config.num_hidden_layers):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "            \n",
    "    def forward(self, x, output_attentions=False):\n",
    "        attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions)\n",
    "            if output_attentions:\n",
    "                attentions.append(attention_probs)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attentions)\n",
    "        \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config.image_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_classes = config.num_classes\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, y, output_attentions=False):\n",
    "        x = self.embeddings(y)\n",
    "        x, attentions = self.encoder(x, output_attentions)\n",
    "        x = x[:, 0,: ]\n",
    "        x = self.classifier(x)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attentions)\n",
    "        \n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.positional_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.positional_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.positional_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config.initializer_range,\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_encoding(label, classes):\n",
    "    one_hot = torch.zeros(len(classes), dtype=torch.float32)\n",
    "    \n",
    "    if label in classes:\n",
    "        index = classes.index(label)\n",
    "        one_hot[index] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFERDataset(Dataset):\n",
    "    def __init__(self, image_parent_directory, data_directory,classes , transform=None):\n",
    "        self.image_parent_directory = image_parent_directory\n",
    "        self.df = pd.read_csv(data_directory)\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_parent_directory + self.df.iloc[idx, 0]\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.df.iloc[idx, 1]\n",
    "        if label in self.classes:\n",
    "            index = self.classes.index(label)\n",
    "        # label_tensor = torch.tensor(create_one_hot_encoding(label, self.classes))\n",
    "        \n",
    "        return image, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData():\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage() ,\n",
    "        v2.ToDtype(torch.uint8, scale=True),\n",
    "        v2.CenterCrop((96, 96)),\n",
    "        v2.ToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    classes = ('anger', 'contempt', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise')\n",
    "    train_data = CustomFERDataset(\"data/archive/\" , \"data/train.csv\" ,classes , transform=transform)\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_data = CustomFERDataset(\"data/archive/\" , \"data/test.csv\" ,classes , transform=transform )\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n",
    "    return train_loader, test_loader ,classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemangjain/Desktop/4th/FER/env/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.7754, -1.7754, -1.7754,  ..., -2.0665, -2.0837, -2.1179],\n",
      "          [-1.7754, -1.7754, -1.7754,  ..., -2.0665, -2.0837, -2.1008],\n",
      "          [-1.7925, -1.7754, -1.7583,  ..., -2.0837, -2.0665, -2.0665],\n",
      "          ...,\n",
      "          [ 0.5193,  0.5536,  0.5878,  ..., -0.3369, -0.3712, -0.4739],\n",
      "          [ 0.5707,  0.6049,  0.5878,  ..., -0.3369, -0.3712, -0.4568],\n",
      "          [ 0.5022,  0.5022,  0.5878,  ..., -0.3198, -0.2856, -0.4054]],\n",
      "\n",
      "         [[-1.6856, -1.6856, -1.6856,  ..., -1.9832, -2.0007, -2.0357],\n",
      "          [-1.6856, -1.6856, -1.6856,  ..., -1.9832, -2.0007, -2.0182],\n",
      "          [-1.7031, -1.6856, -1.6681,  ..., -2.0007, -1.9832, -1.9832],\n",
      "          ...,\n",
      "          [-1.7031, -1.6681, -1.7206,  ..., -0.1975, -0.1800, -0.2850],\n",
      "          [-1.6155, -1.5805, -1.6681,  ..., -0.1975, -0.1975, -0.2850],\n",
      "          [-1.6331, -1.6856, -1.6331,  ..., -0.1800, -0.1450, -0.2325]],\n",
      "\n",
      "         [[-1.4559, -1.4559, -1.4559,  ..., -1.7522, -1.7696, -1.8044],\n",
      "          [-1.4559, -1.4559, -1.4559,  ..., -1.7522, -1.7696, -1.7870],\n",
      "          [-1.4733, -1.4559, -1.4384,  ..., -1.7696, -1.7522, -1.7522],\n",
      "          ...,\n",
      "          [-1.3687, -1.3339, -1.3339,  ...,  0.1128,  0.0953, -0.0092],\n",
      "          [-1.3339, -1.2641, -1.2990,  ...,  0.1128,  0.1128,  0.0256],\n",
      "          [-1.3687, -1.4036, -1.2816,  ...,  0.1302,  0.1651,  0.0779]]],\n",
      "\n",
      "\n",
      "        [[[ 0.5878,  0.5707,  0.5707,  ..., -1.0390,  0.8961,  0.1597],\n",
      "          [ 0.5878,  0.5878,  0.5878,  ..., -0.7479,  0.7419,  0.2453],\n",
      "          [ 0.5707,  0.6049,  0.5536,  ..., -1.0390, -0.1314,  0.6392],\n",
      "          ...,\n",
      "          [-0.1828, -0.1314, -0.1657,  ...,  1.0844, -1.2617, -1.8953],\n",
      "          [-0.2171, -0.2342, -0.1486,  ...,  1.1529,  0.7248, -1.7412],\n",
      "          [-0.2684, -0.2171, -0.1999,  ...,  1.1358,  1.1700,  0.0569]],\n",
      "\n",
      "         [[ 0.4853,  0.4678,  0.4678,  ..., -1.0028,  0.9580,  0.2927],\n",
      "          [ 0.4678,  0.4678,  0.4503,  ..., -0.6702,  0.7654,  0.3452],\n",
      "          [ 0.4503,  0.4853,  0.4328,  ..., -1.0028, -0.0749,  0.6954],\n",
      "          ...,\n",
      "          [-0.8978, -0.9328, -0.8978,  ...,  0.0126, -1.4930, -1.8256],\n",
      "          [-0.8627, -0.8627, -0.8627,  ...,  0.0651, -0.2500, -1.7556],\n",
      "          [-0.9153, -0.8803, -0.9153,  ...,  0.0126,  0.0651, -0.6001]],\n",
      "\n",
      "         [[ 0.5485,  0.5485,  0.6531,  ..., -0.8458,  1.3154,  0.4788],\n",
      "          [ 0.5659,  0.5485,  0.5485,  ..., -0.4798,  1.0539,  0.5311],\n",
      "          [ 0.5485,  0.6008,  0.5311,  ..., -0.7761,  0.1651,  0.9494],\n",
      "          ...,\n",
      "          [-0.2881, -0.3404, -0.3927,  ..., -0.4275, -1.4733, -1.6127],\n",
      "          [-0.2881, -0.3055, -0.3578,  ..., -0.4798, -0.7238, -1.6302],\n",
      "          [-0.3230, -0.3578, -0.3753,  ..., -0.6890, -0.4624, -1.1421]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3309,  0.1939,  0.0569,  ...,  0.0398, -0.0458,  0.0398],\n",
      "          [ 0.4337,  0.1426, -0.0801,  ...,  0.0056,  0.0569,  0.0741],\n",
      "          [ 0.4679,  0.1939, -0.2171,  ..., -0.0287,  0.0741,  0.1083],\n",
      "          ...,\n",
      "          [ 0.6906,  0.7762,  0.8104,  ..., -0.3369, -0.4054, -0.4054],\n",
      "          [ 0.7077,  0.7933,  0.7762,  ..., -0.9877, -0.3541, -0.4054],\n",
      "          [ 0.6906,  0.7762,  0.7419,  ..., -1.2445, -0.2342, -0.3369]],\n",
      "\n",
      "         [[ 0.1702,  0.0301, -0.0924,  ..., -0.0924, -0.1800, -0.0924],\n",
      "          [ 0.2752, -0.0224, -0.2325,  ..., -0.1275, -0.0749, -0.0574],\n",
      "          [ 0.3102,  0.0301, -0.3375,  ..., -0.1625, -0.0574, -0.0224],\n",
      "          ...,\n",
      "          [ 0.5203,  0.6078,  0.6429,  ..., -0.4426, -0.5651, -0.5651],\n",
      "          [ 0.5378,  0.6254,  0.6078,  ..., -1.1078, -0.5126, -0.5651],\n",
      "          [ 0.5203,  0.6078,  0.5728,  ..., -1.3354, -0.3901, -0.4951]],\n",
      "\n",
      "         [[ 0.6705,  0.5311,  0.3568,  ...,  0.4614,  0.3742,  0.4614],\n",
      "          [ 0.7751,  0.4788,  0.2173,  ...,  0.4265,  0.4788,  0.4962],\n",
      "          [ 0.8099,  0.5311,  0.1128,  ...,  0.3916,  0.4962,  0.5311],\n",
      "          ...,\n",
      "          [ 1.1585,  1.2457,  1.2805,  ..., -0.0615, -0.1661, -0.1661],\n",
      "          [ 1.1759,  1.2631,  1.2457,  ..., -0.7238, -0.0964, -0.1487],\n",
      "          [ 1.1585,  1.2457,  1.2108,  ..., -0.9678,  0.0256, -0.0790]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4954,  1.4612,  1.4269,  ..., -0.2684, -0.2856,  0.0912],\n",
      "          [ 1.6667,  1.6495,  1.6153,  ..., -0.3027, -0.5596,  0.0569],\n",
      "          [ 1.8037,  1.8037,  1.8208,  ..., -0.5253, -0.5253, -0.6281],\n",
      "          ...,\n",
      "          [ 1.8550,  1.7694,  1.8722,  ...,  1.1529,  1.0331,  0.8447],\n",
      "          [ 1.3242,  1.4269,  1.3755,  ...,  1.1187,  0.9646,  0.7762],\n",
      "          [ 0.1083,  0.0227,  0.2111,  ...,  1.1015,  0.9303,  0.7248]],\n",
      "\n",
      "         [[ 1.5707,  1.5357,  1.5007,  ..., -0.8627, -0.8277, -0.4426],\n",
      "          [ 1.7458,  1.7283,  1.6933,  ..., -0.8452, -1.1078, -0.4776],\n",
      "          [ 1.9034,  1.9034,  1.9034,  ..., -1.0728, -1.0728, -1.1779],\n",
      "          ...,\n",
      "          [ 2.2010,  2.1134,  2.2535,  ...,  1.0280,  0.9055,  0.7129],\n",
      "          [ 2.3060,  2.3761,  2.2535,  ...,  1.0455,  0.8880,  0.6954],\n",
      "          [ 1.3957,  1.2381,  1.3606,  ...,  1.0280,  0.8529,  0.6429]],\n",
      "\n",
      "         [[ 1.7337,  1.6988,  1.6465,  ..., -0.9853, -0.9678, -0.5844],\n",
      "          [ 1.9080,  1.8905,  1.8383,  ..., -0.9853, -1.2467, -0.6193],\n",
      "          [ 2.0648,  2.0648,  2.0474,  ..., -1.1770, -1.2119, -1.3164],\n",
      "          ...,\n",
      "          [ 2.4309,  2.3088,  2.3960,  ...,  0.9668,  0.8448,  0.6531],\n",
      "          [ 2.6400,  2.6400,  2.5006,  ...,  0.9842,  0.8274,  0.6356],\n",
      "          [ 1.8557,  1.6465,  1.6465,  ...,  0.9668,  0.7925,  0.5834]]],\n",
      "\n",
      "\n",
      "        [[[ 1.6324,  1.6324,  1.6495,  ...,  1.7523,  1.7352,  1.7352],\n",
      "          [ 1.6495,  1.6495,  1.6667,  ...,  1.7523,  1.7523,  1.7523],\n",
      "          [ 1.6495,  1.6667,  1.6667,  ...,  1.7694,  1.7694,  1.7694],\n",
      "          ...,\n",
      "          [ 1.4612,  1.4783,  1.4954,  ...,  1.6153,  1.6153,  1.5982],\n",
      "          [ 1.4440,  1.4612,  1.4783,  ...,  1.5982,  1.5982,  1.5982],\n",
      "          [ 1.4269,  1.4440,  1.4440,  ...,  1.5982,  1.5810,  1.5810]],\n",
      "\n",
      "         [[ 1.8158,  1.8158,  1.8333,  ...,  1.9384,  1.9209,  1.9209],\n",
      "          [ 1.8333,  1.8333,  1.8508,  ...,  1.9384,  1.9384,  1.9384],\n",
      "          [ 1.8333,  1.8508,  1.8508,  ...,  1.9559,  1.9559,  1.9559],\n",
      "          ...,\n",
      "          [ 1.6408,  1.6583,  1.6758,  ...,  1.7983,  1.7983,  1.7808],\n",
      "          [ 1.6232,  1.6408,  1.6583,  ...,  1.7808,  1.7808,  1.7808],\n",
      "          [ 1.6057,  1.6232,  1.6232,  ...,  1.7808,  1.7633,  1.7633]],\n",
      "\n",
      "         [[ 2.0648,  2.0648,  2.0823,  ...,  2.1868,  2.1694,  2.1694],\n",
      "          [ 2.0823,  2.0823,  2.0997,  ...,  2.1868,  2.1868,  2.1868],\n",
      "          [ 2.0823,  2.0997,  2.0997,  ...,  2.2043,  2.2043,  2.2043],\n",
      "          ...,\n",
      "          [ 1.9428,  1.9603,  1.9603,  ...,  2.0823,  2.0823,  2.0648],\n",
      "          [ 1.9080,  1.9254,  1.9428,  ...,  2.0648,  2.0648,  2.0648],\n",
      "          [ 1.8905,  1.9080,  1.9080,  ...,  2.0648,  2.0474,  2.0474]]],\n",
      "\n",
      "\n",
      "        [[[-1.0904, -0.8849, -0.6452,  ..., -0.1143,  0.0398,  0.1939],\n",
      "          [-1.0219, -0.5767, -0.6109,  ..., -0.2171, -0.0287,  0.1083],\n",
      "          [-0.3883, -0.6452, -0.7822,  ..., -0.3541, -0.1657, -0.0116],\n",
      "          ...,\n",
      "          [-1.2617, -1.2103, -1.3473,  ..., -1.2788, -1.1418, -1.0562],\n",
      "          [-1.2274, -1.2959, -1.2959,  ..., -1.2959, -1.1589, -1.0733],\n",
      "          [-1.2788, -1.3302, -1.2959,  ..., -1.3302, -1.1932, -1.0904]],\n",
      "\n",
      "         [[-1.0903, -0.8627, -0.7052,  ...,  0.6429,  0.7654,  0.9230],\n",
      "          [-1.0553, -0.6001, -0.6702,  ...,  0.5378,  0.6954,  0.8354],\n",
      "          [-0.4251, -0.6877, -0.8277,  ...,  0.3803,  0.5553,  0.7129],\n",
      "          ...,\n",
      "          [-0.8978, -0.8452, -1.0028,  ..., -1.3529, -1.2479, -1.1779],\n",
      "          [-0.8452, -0.9153, -0.9328,  ..., -1.3704, -1.2654, -1.1954],\n",
      "          [-0.8627, -0.9153, -0.9153,  ..., -1.4055, -1.3004, -1.2129]],\n",
      "\n",
      "         [[-0.8633, -0.6890, -0.4973,  ...,  1.4897,  1.5942,  1.7511],\n",
      "          [-0.8633, -0.4275, -0.4798,  ...,  1.3851,  1.5594,  1.6640],\n",
      "          [-0.2881, -0.5844, -0.7238,  ...,  1.2631,  1.4374,  1.5768],\n",
      "          ...,\n",
      "          [-0.1661, -0.1138, -0.2707,  ..., -1.2990, -1.2641, -1.2293],\n",
      "          [-0.0267, -0.0964, -0.1312,  ..., -1.3513, -1.2816, -1.2467],\n",
      "          [-0.0092, -0.0615, -0.0964,  ..., -1.3861, -1.3164, -1.2641]]]]), tensor([3, 4, 7, 7, 7, 6, 2, 2, 1, 5, 6, 7, 7, 4, 1, 4, 4, 7, 3, 5, 6, 4, 3, 1,\n",
      "        1, 2, 1, 5, 1, 1, 0, 2])]\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader ,classes = prepareData()\n",
    "\n",
    "for _ ,batch in enumerate(train_loader):\n",
    "    if _==0:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(experiment_name, model, epoch, base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    cpfile = os.path.join(outdir, f'model_{epoch}.pt')\n",
    "    torch.save(model.state_dict(), cpfile)\n",
    "\n",
    "def save_experiment(experiment_name , config , model , train_losses , test_losses , accuracies , base_dir= \"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    configfile = os.path.join(outdir, 'config.json')\n",
    "    with open(configfile, 'w') as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "        \n",
    "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "    with open(jsonfile, 'w') as f:\n",
    "        data = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'accuracies': accuracies,\n",
    "        }\n",
    "        json.dump(data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(experiment_name, model, \"final\", base_dir=base_dir)\n",
    "    \n",
    "def load_experiment(experiment_name, checkpoint_name=\"model_final.pt\", base_dir=\"experiments\"):\n",
    "    outdir = os.path.join(base_dir, experiment_name)\n",
    "    configfile = os.path.join(outdir, 'config.json')\n",
    "    with open(configfile, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
    "    with open(jsonfile, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    train_losses = data['train_losses']\n",
    "    test_losses = data['test_losses']\n",
    "    accuracies = data['accuracies']\n",
    "    model = VisionTransformer(config)\n",
    "    cpfile = os.path.join(outdir, checkpoint_name)\n",
    "    model.load_state_dict(torch.load(cpfile))\n",
    "    return config, model, train_losses, test_losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}({self.__dict__})\"\n",
    "    \n",
    "config_dict = {\n",
    "    \"patch_size\": 4,  \n",
    "    \"hidden_size\": 64,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 4 * 64,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"initializer_range\": 0.04,\n",
    "    \"image_size\": 96,\n",
    "    \"num_classes\": 8,\n",
    "    \"num_channels\": 3,\n",
    "    \"in_channels\": 3,\n",
    "    \"qkv_bias\": True,\n",
    "}\n",
    "\n",
    "config = Config(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"vision_transformer\"\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "save_model_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self , model , optimizer , loss_fn , exp_name):\n",
    "        self.model = model \n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.exp_name = exp_name\n",
    "        \n",
    "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=2):\n",
    "        train_losses, test_losses, accuracies = [], [], []\n",
    "        for i in range(epochs):\n",
    "            train_loss = self.train_epoch(trainloader)\n",
    "            accuracy, test_loss = self.evaluate(testloader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.exp_name, self.model, i+1)\n",
    "                \n",
    "        save_experiment(self.exp_name, config, self.model, train_losses, test_losses, accuracies)\n",
    "        \n",
    "    def train_epoch(self, trainloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(trainloader ,  total=len(train_loader)):\n",
    "            images, labels = batch\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss_fn(self.model(images)[0], labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item() * len(images)\n",
    "        return total_loss / len(trainloader.dataset)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                images, labels = batch\n",
    "                logits, _ = self.model(images)\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hemangjain/Desktop/4th/FER/env/lib/python3.12/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "100%|██████████| 705/705 [19:11<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 2.0715, Test loss: 2.0653, Accuracy: 0.1561\n",
      "\tSave checkpoint at epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [19:06<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 2.0587, Test loss: 2.0602, Accuracy: 0.1838\n",
      "\tSave checkpoint at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [23:42<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 2.0198, Test loss: 1.9469, Accuracy: 0.2562\n",
      "\tSave checkpoint at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [22:46<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.8844, Test loss: 1.8190, Accuracy: 0.3165\n",
      "\tSave checkpoint at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [22:26<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.8028, Test loss: 1.7643, Accuracy: 0.3201\n",
      "\tSave checkpoint at epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [22:29<00:00,  1.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.7513, Test loss: 1.7549, Accuracy: 0.3314\n",
      "\tSave checkpoint at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [22:20<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.7140, Test loss: 1.7136, Accuracy: 0.3463\n",
      "\tSave checkpoint at epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [26:09<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.6835, Test loss: 1.6544, Accuracy: 0.3730\n",
      "\tSave checkpoint at epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [29:26<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 1.6556, Test loss: 1.6691, Accuracy: 0.3659\n",
      "\tSave checkpoint at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705/705 [29:15<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 1.6291, Test loss: 1.6078, Accuracy: 0.3879\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Config is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[170], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mtrain(trainloader, testloader, epochs, save_model_every_n_epochs\u001b[38;5;241m=\u001b[39msave_model_every_n_epochs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[170], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, loss_fn, exp_name)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[169], line 21\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, trainloader, testloader, epochs, save_model_every_n_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mSave checkpoint at epoch\u001b[39m\u001b[38;5;124m'\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m         save_checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexp_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m \u001b[43msave_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_losses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracies\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[166], line 12\u001b[0m, in \u001b[0;36msave_experiment\u001b[0;34m(experiment_name, config, model, train_losses, test_losses, accuracies, base_dir)\u001b[0m\n\u001b[1;32m     10\u001b[0m configfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(configfile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m jsonfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outdir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(jsonfile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/Desktop/4th/FER/env/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/4th/FER/env/lib/python3.12/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/4th/FER/env/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Config is not JSON serializable"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    save_model_every_n_epochs = save_model_every\n",
    "    trainloader, testloader, _ = prepareData()\n",
    "    model = VisionTransformer(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, loss_fn, exp_name)\n",
    "    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
